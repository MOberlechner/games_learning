{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from games_learning.game.matrix_game import MatrixGame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce the three main concepts and classes:\n",
    "- Matrix Games\n",
    "- Strategies\n",
    "- Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic structure we use are matrix games. Matrix games can be simply defined by defining a payoff-matrix (e.g. battle of sexes) or constructed through discretization from some continuous game (e.g., auctions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MatrixGame(example,[3, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payoff_matrix = np.array([\n",
    "        [[1, -2, 3], [3, -1, 2], [2, 1, 3]],   \n",
    "        [[-1, 2, 1], [-3, 4, 2], [3, 1, 3]]\n",
    "     ])\n",
    "game = MatrixGame(payoff_matrix=payoff_matrix, name=\"example\")\n",
    "game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access basic properties such as number of agents, number of actions, and the payoff matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Agents: 2\n",
      "Number of Actions: [3, 3]\n",
      "Payoff Matrices:\n",
      " [[[ 1 -2  3]\n",
      "  [ 3 -1  2]\n",
      "  [ 2  1  3]]\n",
      "\n",
      " [[-1  2  1]\n",
      "  [-3  4  2]\n",
      "  [ 3  1  3]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Agents:\", game.n_agents)\n",
    "print(\"Number of Actions:\", game.n_actions)\n",
    "print(\"Payoff Matrices:\\n\", game.payoff_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different methods to analyze games:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weak_ne': [(2, 2)], 'strict_ne': [], 'ne': [(2, 2)]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can compute all pure Nash equilibria\n",
    "# note that ne = weak_ne + strict_ne\n",
    "game.get_pne()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weak_ne': [('C', 'F')], 'strict_ne': [], 'ne': [('C', 'F')]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we provide a list of names for the actions, the ouput gives us the names instead of the numbers\n",
    "game = MatrixGame(payoff_matrix=payoff_matrix, name=\"example\", name_actions=[[\"A\", \"B\", \"C\"], [\"D\", \"E\", \"F\"]])\n",
    "pne = game.get_pne()\n",
    "game.get_named_actions(pne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCE: \n",
      " [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 1.]]\n",
      "CE:\n",
      " [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# We compute correlated and coarse correlated equilibria \n",
    "print(\"CCE: \\n\", game.get_cce())\n",
    "print(\"CE:\\n\", game.get_ce())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE:\n",
      " [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 1.]]\n",
      "CCE:\n",
      " [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# If wou want to check, if other actions can be supported by a (C)CE, you can either define a respective objective\n",
    "c = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n",
    "print(\"CE:\\n\", game.get_ce(objective=c))\n",
    "print(\"CCE:\\n\", game.get_cce(objective=c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False,  True],\n",
       "       [False, False, False],\n",
       "       [ True, False,  True]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can call a function to compute the support of all possible CCEs \n",
    "# (note that this corresponds to #actions ^ #agents computations of CCE)\n",
    "game.get_supp_cce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3) --> (3, 3)\n",
      "Actions Agent 0: 0 1 2\n",
      "Actions Agent 1: 0 1 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: [0, 1, 2], 1: [0, 1, 2]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also determine actions that are serially undominated (su)\n",
    "# note: if you are interested in the reduced game, use the method in games.learning/utils/dominance.py \n",
    "su_actions = game.get_undominated_actions(dominance=\"strict\", print=True)\n",
    "su_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Games\n",
    "\n",
    "Other ways to create matrix game are given by the following classes:\n",
    "- *ExampleMatrixGames*: classical examples such as matching_pennis, prisoners_dilemma, ...\n",
    "- *RandomMatrixGames*: sample entries of payoff matrix\n",
    "- *EconGames*: discretized version of complete-information economic games, e.g., auctions, contests, oligopolies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from games_learning.game.matrix_game import ExampleMatrixGames\n",
    "from games_learning.game.econ_game import FPSB, AllPay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False],\n",
       "       [False,  True]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Prisoners' Dilemma\n",
    "game1 = ExampleMatrixGames(setting=\"prisoners_dilemma\")\n",
    "game1.get_supp_cce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pure NE (numbered): {'weak_ne': [(17, 17)], 'strict_ne': [(18, 18)], 'ne': [(18, 18), (17, 17)]}\n",
      "pure NE (named)   : {'weak_ne': [('0.90', '0.90')], 'strict_ne': [('0.95', '0.95')], 'ne': [('0.95', '0.95'), ('0.90', '0.90')]}\n",
      "\n",
      "Serially strict undominated actions:\n",
      "(19, 19) --> (2, 2)\n",
      "Actions Agent 0: X X X X X X X X X X X X X X X X X 17 18\n",
      "Actions Agent 1: X X X X X X X X X X X X X X X X X 17 18\n"
     ]
    }
   ],
   "source": [
    "# Example First-Price Sealed-Bid Auction\n",
    "game2 = FPSB(n_agents=2, n_discr=19, valuations=(1.0, 1.0), interval=(0.05, 0.95))\n",
    "print(\"pure NE (numbered):\", game2.get_pne())\n",
    "print(\"pure NE (named)   :\", game2.get_named_actions(game2.get_pne()))\n",
    "print(\"\\nSerially strict undominated actions:\")\n",
    "undom_actions = game2.get_undominated_actions(dominance=\"strict\", print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pure NE: []\n",
      "\n",
      "Serially strict undominated actions:\n",
      "(7, 7) --> (7, 7)\n",
      "Actions Agent 0: 0 1 2 3 4 5 6\n",
      "Actions Agent 1: 0 1 2 3 4 5 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: ['0.05', '0.20', '0.35', '0.50', '0.65', '0.80', '0.95'],\n",
       " 1: ['0.05', '0.20', '0.35', '0.50', '0.65', '0.80', '0.95']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game3 = AllPay(n_agents=2, n_discr=7, valuations=(1.0, 1.0), interval=(0.05, 0.95))\n",
    "print(\"pure NE:\", game3.get_pne()[\"ne\"])\n",
    "print(\"\\nSerially strict undominated actions:\")\n",
    "undom_actions = game3.get_undominated_actions(dominance=\"strict\", print=True)\n",
    "game3.get_named_actions(undom_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies\n",
    "\n",
    "Especially in the context of learning algorithms, we look at the mixed extension of the game, i.e., we consider the action set of mixed strategies. These allow us to compute the expected utility, gradient, etc. for any mixed strategy profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from games_learning.strategy import Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategies: Strategies(game:example, [3, 3])\n",
      "Current values of s:\n",
      " [array([0.33333333, 0.33333333, 0.33333333]), array([0.33333333, 0.33333333, 0.33333333])]\n"
     ]
    }
   ],
   "source": [
    "s = Strategy(game, init_method = \"equal\")\n",
    "print(\"Strategies:\", s)\n",
    "print(\"Current values of s:\\n\", s.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Agent 0: [0.66666667 1.33333333 2.        ]\n"
     ]
    }
   ],
   "source": [
    "# Given the mixed strategy, we can compute the gradient for an agent\n",
    "grad0 = s.gradient(agent=0)\n",
    "print(\"Gradient Agent 0:\", grad0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.66666667, 1.33333333, 2.        ]),\n",
       " array([-0.33333333,  2.33333333,  2.        ])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a fast way to access the current gradient for all agents is\n",
    "s.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Utility     : 1.333\n",
      "Absolute Utility Loss: 0.667\n"
     ]
    }
   ],
   "source": [
    "# given the gradient we can compute the expected utility, best response and utility loss of an agent:\n",
    "exp_util = s.utility(agent=0)\n",
    "abs_util_los = s.utility_loss(agent=0, method=\"abs\")\n",
    "print(f\"Expected Utility     : {exp_util:.3f}\")\n",
    "print(f\"Absolute Utility Loss: {abs_util_los:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.best_response(agent=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the computation of the utility, best response, and utility loss, all require the gradient. \n",
    "# To speed up the computation, you can reuse the already computed gradient:\n",
    "%timeit exp_util = s.utility(agent=0)\n",
    "%timeit exp_util = s.utility(agent=0, gradient=grad0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner\n",
    "\n",
    "The learner class provides us with a (gradient-based) method to update strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from games_learning.learner.mirror_ascent import MirrorAscent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = MirrorAscent(eta=2, beta=0.05, mirror_map=\"entropic\")\n",
    "gradients = s.y \n",
    "x_new = learner.update(s, gradients, iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "plt.bar(np.arange(3)-0.2, s.x[0], width=0.37, label=\"$x_t$\")\n",
    "plt.bar(np.arange(3)+0.2, x_new[0], width=0.37, label=\"$x_{t+1}$\")\n",
    "plt.xticks(range(3), range(1, 4))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation\n",
    "\n",
    "To run simulations where agents (simultaneously or sequentially) update their strategy using some learning algorithm, we can use the simulator class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from games_learning.simulation import Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m game \u001b[38;5;241m=\u001b[39m \u001b[43mExampleMatrixGames\u001b[49m\u001b[43m(\u001b[49m\u001b[43msetting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrock_paper_scissors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m strategy \u001b[38;5;241m=\u001b[39m Strategy(game, init_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_sym\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m learner \u001b[38;5;241m=\u001b[39m MirrorAscent(eta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.06\u001b[39m, beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, mirror_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentropic\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/projects/Games_Learning/games_learning/game/matrix_game.py:211\u001b[0m, in \u001b[0;36mExampleMatrixGames.__init__\u001b[0;34m(self, setting, parameter)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    208\u001b[0m     setting: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    209\u001b[0m     parameter: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    210\u001b[0m ):\n\u001b[0;32m--> 211\u001b[0m     payoff_matrix, name, name_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_setting(setting, parameter)\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(payoff_matrix, name, name_actions)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "game = ExampleMatrixGames(setting=\"rock_paper_scissors\")\n",
    "strategy = Strategy(game, init_method=\"uniform_sym\")\n",
    "learner = MirrorAscent(eta=0.06, beta=0.0, mirror_map=\"entropic\")\n",
    "sim = Simulator(strategy, learner, max_iter=1_000)\n",
    "sim.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = 1\n",
    "plt.figure(figsize=(8,3))\n",
    "for a in range(strategy.n_actions[agent]):\n",
    "    plt.plot([sim.log_data[\"strategies\"][agent][t][a] for t in range(1_000)])\n",
    "\n",
    "plt.xlabel(\"Iterations\"); plt.ylim(0,1)\n",
    "plt.title(\"Agent 0: Probabilities of Actions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
