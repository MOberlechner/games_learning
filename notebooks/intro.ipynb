{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from games_learning.game.matrix_game import MatrixGame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce the three main concepts and classes:\n",
    "- Matrix Games\n",
    "- Strategies\n",
    "- Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic structure we use are matrix games. Matrix games can be simply defined by defining a payoff-matrix (e.g. battle of sexes) or constructed through discretization from some continuous game (e.g., auctions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MatrixGame(example,[3, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payoff_matrix = np.array([\n",
    "        [[1, -2, 3], [3, -1, 2], [2, 1, 3]],   \n",
    "        [[-1, 2, 1], [-3, 4, 2], [3, 1, 3]]\n",
    "     ])\n",
    "game = MatrixGame(payoff_matrix=payoff_matrix, name=\"example\")\n",
    "game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access basic properties such as number of agents, number of actions, and the payoff matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Agents: 2\n",
      "Number of Actions: [3, 3]\n",
      "Payoff Matrices:\n",
      " [[[ 1 -2  3]\n",
      "  [ 3 -1  2]\n",
      "  [ 2  1  3]]\n",
      "\n",
      " [[-1  2  1]\n",
      "  [-3  4  2]\n",
      "  [ 3  1  3]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Agents:\", game.n_agents)\n",
    "print(\"Number of Actions:\", game.n_actions)\n",
    "print(\"Payoff Matrices:\\n\", game.payoff_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different methods to analyze games:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weak_ne': [(2, 2)], 'strict_ne': [], 'ne': [(2, 2)]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can compute all pure Nash equilibria\n",
    "# note that ne = weak_ne + strict_ne\n",
    "game.get_pne()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weak_ne': [('C', 'F')], 'strict_ne': [], 'ne': [('C', 'F')]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we provide a list of names for the actions, the ouput gives us the names instead of the numbers\n",
    "game = MatrixGame(payoff_matrix=payoff_matrix, name=\"example\", name_actions=[[\"A\", \"B\", \"C\"], [\"D\", \"E\", \"F\"]])\n",
    "pne = game.get_pne()\n",
    "game.get_named_actions(pne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCE: \n",
      " [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 1.]]\n",
      "CE:\n",
      " [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# We compute correlated and coarse correlated equilibria \n",
    "print(\"CCE: \\n\", game.get_cce())\n",
    "print(\"CE:\\n\", game.get_ce())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE:\n",
      " [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 1.]]\n",
      "CCE:\n",
      " [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# If wou want to check, if other actions can be supported by a (C)CE, you can either define a respective objective\n",
    "c = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n",
    "print(\"CE:\\n\", game.get_ce(objective=c))\n",
    "print(\"CCE:\\n\", game.get_cce(objective=c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False,  True],\n",
       "       [False, False, False],\n",
       "       [ True, False,  True]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can call a function to compute the support of all possible CCEs \n",
    "# (note that this corresponds to #actions ^ #agents computations of CCE)\n",
    "game.get_supp_cce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3) --> (3, 3)\n",
      "Actions Agent 0: 0 1 2\n",
      "Actions Agent 1: 0 1 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: ['A', 'B', 'C'], 1: ['D', 'E', 'F']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also determine actions that are serially undominated (su)\n",
    "# note: if you are interested in the reduced game, use the method in games.learning/utils/dominance.py \n",
    "su_actions = game.get_undominated_actions(dominance=\"strict\", print=True)\n",
    "su_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Games\n",
    "\n",
    "Other ways to create matrix game are given by the following classes:\n",
    "- *ExampleMatrixGames*: classical examples such as matching_pennis, prisoners_dilemma, ...\n",
    "- *RandomMatrixGames*: sample entries of payoff matrix\n",
    "- *EconGames*: discretized version of complete-information economic games, e.g., auctions, contests, oligopolies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from games_learning.game.matrix_game import ExampleMatrixGames\n",
    "from games_learning.game.econ_game import FPSB, AllPay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Prisoners' Dilemma\n",
    "game1 = ExampleMatrixGames(setting=\"prisoners_dilemma\")\n",
    "game1.get_supp_cce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example First-Price Sealed-Bid Auction\n",
    "game2 = FPSB(n_agents=2, n_discr=19, valuations=(1.0, 1.0), interval=(0.05, 0.95))\n",
    "print(\"pure NE:\", game2.get_pne())\n",
    "print(\"\\nSerially strict undominated actions:\")\n",
    "undom_actions = game2.get_undominated_actions(dominance=\"strict\", print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game3 = AllPay(n_agents=2, n_discr=10, valuations=(1.0, 1.0), interval=(0.05, 0.95))\n",
    "print(\"pure NE:\", game3.get_pne()[\"ne\"])\n",
    "print(\"\\nSerially strict undominated actions:\")\n",
    "undom_actions = game3.get_undominated_actions(dominance=\"strict\", print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies\n",
    "\n",
    "Especially in the context of learning algorithms, we look at the mixed extension of the game, i.e., we consider the action set of mixed strategies. These allow us to compute the expected utility, gradient, etc. for any mixed strategy profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from games_learning.strategy import Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Strategy(game, init_method = \"equal\")\n",
    "print(\"Strategies:\", s)\n",
    "print(\"Current values of s:\\n\", s.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the mixed strategy, we can compute the gradient for an agent\n",
    "grad0 = s.gradient(agent=0)\n",
    "print(\"Gradient Agent 0:\", grad0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a fast way to access the current gradient for all agents is\n",
    "s.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given the gradient we can compute the expected utility, best response and utility loss of an agent:\n",
    "exp_util = s.utility(agent=0)\n",
    "abs_util_los = s.utility_loss(agent=0, method=\"abs\")\n",
    "print(f\"Expected Utility     : {exp_util:.3f}\")\n",
    "print(f\"Absolute Utility Loss: {abs_util_los:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.best_response(agent=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the computation of the utility, best response, and utility loss, all require the gradient. \n",
    "# To speed up the computation, you can reuse the already computed gradient:\n",
    "%timeit exp_util = s.utility(agent=0)\n",
    "%timeit exp_util = s.utility(agent=0, gradient=grad0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner\n",
    "\n",
    "The learner class provides us with a (gradient-based) method to update strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from games_learning.learner.mirror_ascent import MirrorAscent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = MirrorAscent(eta=2, beta=0.05, mirror_map=\"entropic\")\n",
    "gradients = s.y \n",
    "x_new = learner.update(s, gradients, iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "plt.bar(np.arange(3)-0.2, s.x[0], width=0.37, label=\"$x_t$\")\n",
    "plt.bar(np.arange(3)+0.2, x_new[0], width=0.37, label=\"$x_{t+1}$\")\n",
    "plt.xticks(range(3), range(1, 4))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation\n",
    "\n",
    "To run simulations where agents (simultaneously or sequentially) update their strategy using some learning algorithm, we can use the simulator class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from games_learning.simulation import Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = ExampleMatrixGames(setting=\"rock_paper_scissors\")\n",
    "strategy = Strategy(game, init_method=\"uniform_sym\")\n",
    "learner = MirrorAscent(eta=0.06, beta=0.0, mirror_map=\"entropic\")\n",
    "sim = Simulator(strategy, learner, max_iter=1_000)\n",
    "sim.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = 1\n",
    "plt.figure(figsize=(8,3))\n",
    "for a in range(strategy.n_actions[agent]):\n",
    "    plt.plot([sim.log_data[\"strategies\"][agent][t][a] for t in range(1_000)])\n",
    "\n",
    "plt.xlabel(\"Iterations\"); plt.ylim(0,1)\n",
    "plt.title(\"Agent 0: Probabilities of Actions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
